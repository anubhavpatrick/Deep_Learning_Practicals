{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from IPython.display import clear_output#to clear output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Loading Data\n",
    "2. Import keras models\n",
    "3. Add different layers using model.add<br>\n",
    "Densely connected layer is one in which each neuron in every layer is connected to every other neuron in next layer\n",
    "4. Compile Model\n",
    "5. Fit model\n",
    "6. Evaluate Model\n",
    "7. Save model\n",
    "8. Load Model\n",
    "9. Predict using model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data \n",
    "\n",
    "Neural Network works best when data in entire datasent in scaled on some scale\n",
    "Pandas act as though we are working in a <b>virtual spreadsheet</b>.We can do many of the same functions that we can do on a spreadsheet like sorting data, pulling out certain columns of data and or applying calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "training_data_df=pd.read_csv(\"sales_data_training.csv\")#training data data frame\n",
    "test_data_df=pd.read_csv(\"sales_data_test.csv\")#testing data data frame\n",
    "print(training_data_df)\n",
    "time.sleep(5)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "<b>minmax</b> object of sklearn library will help to scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.33333333e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 3.69678934e-06 5.00000000e-02]\n",
      "Note: total_earnings values were scaled by multiplying by 0.0000036968 and adding -0.115913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))#setting the range of scaling\n",
    "scaled_training = scaler.fit_transform(training_data_df)#fit_transform returns plain numpy array and not pandas dataframe\n",
    "scaled_testing = scaler.transform(test_data_df)#use the parameters computed above for scaling data\n",
    "#finding how much each column is scaled in training\n",
    "print(scaler.scale_)\n",
    "#finding how much total training data is scaled\n",
    "print(\"Note: total_earnings values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "#reconverting numpy array scaled_training into pandas dataframe so that it can be written to file easily\n",
    "scaled_training_df=pd.DataFrame(scaled_training,columns=training_data_df.columns.values)\n",
    "scaled_testing_df=pd.DataFrame(scaled_testing,columns=training_data_df.columns.values)\n",
    "\n",
    "#writing scaled training and testing data to .csv file\n",
    "scaled_training_df.to_csv(\"sales_data_training_scaled.csv\",index=False)\n",
    "scaled_testing_df.to_csv(\"sales_data_testing_scaled.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing training data into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaled_training_df.drop(columns='total_earnings',axis=1).values\n",
    "Y=scaled_training_df[['total_earnings']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing keras, creating model, compiling model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(32,input_dim=9,activation='relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(1,activation='linear'))#very imp to choose correct activation function. Act. fun is not sigmoid because this prob is not of logistic regression\n",
    "model.compile(optimizer='adam',loss='mse')#adam is one of the gradient descent mechanism\n",
    "#MSE (Y_actual(i)^2-Y_predicted(i)^2)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30ef1f60>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X,\n",
    "    Y, \n",
    "    epochs=50,  \n",
    "    shuffle=True,\n",
    "    verbose=0)\n",
    "#X: actual data for prediction\n",
    "#Y: actual output values\n",
    "#epochs: a single training pass across the entire training data is called epoch\n",
    "#verbose: print more detaled model about what is going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Model is evaluated on <b>test data</b>\n",
    "\n",
    "### Dividing test data into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_testing_df=pd.read_csv(\"sales_data_testing_scaled.csv\")\n",
    "X_test=scaled_testing_df.drop(columns='total_earnings',axis=1).values\n",
    "Y_test=scaled_testing_df[['total_earnings']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model on training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.00038611555704846977\n"
     ]
    }
   ],
   "source": [
    "test_error_rate=model.evaluate(X_test,Y_test,verbose=0)#the smaller the error rate, the better it is\n",
    "print(\"MSE is {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Predict = pd.read_csv('proposed_new_product.csv').values\n",
    "prediction = model.predict(X_Predict)\n",
    "#We only want first element of the first prediction\n",
    "Y_Predict = prediction[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling back data is very imp. The scaling operations must be performed in reverse order### Multiplication becomes division and addition becomes subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted total earning of proposed product 224965.80995600738\n"
     ]
    }
   ],
   "source": [
    "#Scaling back predicted values from [0, 1] to original scale\n",
    "Y_Predict_Scaled=Y_Predict / 0.0000036968+ 0.115913\n",
    "print('Predicted total earning of proposed product {}'.format(Y_Predict_Scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained model\n",
    "large neural networks takes hours/days in training. We cannot retrain them for each time for execution. We have to save the trained model and load the trained model if and when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model.h5')#save the complete model\n",
    "#h5 is binary file format used to store python array objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring that model is working fine after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted total earning of proposed product 224965.80995600738\n"
     ]
    }
   ],
   "source": [
    "#testing whether loaded model is working correctly or not\n",
    "X_Predict = pd.read_csv('proposed_new_product.csv').values\n",
    "prediction = loaded_model.predict(X_Predict)\n",
    "#We only want first element of the first prediction\n",
    "Y_Predict = prediction[0][0]\n",
    "#Scaling back predicted values from [0, 1] to original scale\n",
    "Y_Predict_Scaled=Y_Predict / 0.0000036968+ 0.115913\n",
    "print('Predicted total earning of proposed product {}'.format(Y_Predict_Scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
